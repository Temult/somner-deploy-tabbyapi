Directory Structure:

└── ./
    ├── .gitignore
    ├── api_tokens.yml.sample
    ├── build.sh.sample
    ├── Caddyfile
    ├── config.yml.sample
    ├── Dockerfile
    ├── README.md
    ├── start_services.sh
    └── tailscale-acl.json.sample



---
File: /.gitignore
---

# Ignore the actual tokens file. Users should create this from the sample.
api_tokens.yml

# Ignore the user-specific build script. The .sample file is tracked instead.
build.sh

# Ignore the user-specific configuration file. The .sample file is tracked instead.
config.yml

# Ignore common Python cache files
__pycache__/
*.pyc
*.pyo
*.pyd

# Ignore common OS-specific files
.DS_Store
Thumbs.db


---
File: /api_tokens.yml.sample
---

api_key: REPLACE_ME
admin_key: REPLACE_ME


---
File: /build.sh.sample
---

#!/usr/bin/env bash
# ==============================================================================
# Docker Build Script for Somner-Deploy-TabbyAPI (Sample)
# ==============================================================================
#
# SUMMARY:
# This script provides a reproducible and easy-to-use method for building the
# project's Docker container. It encapsulates all necessary build arguments,
# making the build process consistent, transparent, and less error-prone.
#
# USAGE:
# 1. Copy this file to `build.sh`: cp build.sh.sample build.sh
# 2. Edit `build.sh` to set your Docker Hub username.
# 3. Make the script executable: `chmod +x build.sh`
# 4. Run it from the repository root: `./build.sh`
#
# AI-NOTE: For Windows users, `chmod` is not required. Use a bash-compatible
# terminal (like Git Bash) and run the script with: `bash build.sh`
#
# ==============================================================================

# ------------------------------------------------------------------------------
# Script Configuration and Safety
# ------------------------------------------------------------------------------
# RATIONALE: `set -e` ensures that the script will exit immediately if any
# command fails. This prevents the script from continuing on an error, for
# example, if Docker is not running or a build argument is misspelled.
set -e

# ==============================================================================
# SECTION: Configuration
# PURPOSE: Define all variables used in the build process. This centralizes
#          configuration, making it easy to update dependency versions or
#          retag the image without searching through command-line flags.
# ==============================================================================

# --- Docker Image Identification ---
# RATIONALE: Defines the Docker Hub username, the name of the image, and the tag.
# Modifying these allows you to build the image for a different repository or version.
#
# ACTION REQUIRED: Replace "yourusername" with your actual Docker Hub username.
DOCKER_USERNAME="yourusername"
IMAGE_NAME="somner"
TAG="dev1"

# --- Build Argument Versions ---
# AI-NOTE: These variables correspond directly to the `ARG` instructions defined
# in the Dockerfile. Changing a version here will inject it into the build process
# without requiring any modifications to the Dockerfile's logic.
CUDA_VERSION="12.8.0"
PYTHON_VERSION="3.11"
TORCH_VERSION="2.7.1+cu128"
TORCHVISION_VERSION="0.22.1+cu128"
TORCHAUDIO_VERSION="2.7.1+cu128"
CADDY_VERSION="2.7.6"

# ==============================================================================
# SECTION: Build Execution
# PURPOSE: Orchestrates the container build using the configuration defined above.
# ==============================================================================

echo "[INFO] Starting Docker build for image: ${DOCKER_USERNAME}/${IMAGE_NAME}:${TAG}"
echo "[INFO] Using Dockerfile: Dockerfile"

# RATIONALE: We explicitly pass every version as a `--build-arg`. This makes the
# build process transparent and highly reproducible. The Dockerfile is treated as
# a template, and this script provides the concrete values, ensuring that anyone
# running this script will produce an identical environment.
docker build \
  --tag "${DOCKER_USERNAME}/${IMAGE_NAME}:${TAG}" \
  --build-arg CUDA_VERSION="${CUDA_VERSION}" \
  --build-arg PYTHON_VERSION="${PYTHON_VERSION}" \
  --build-arg TORCH_VERSION="${TORCH_VERSION}" \
  --build-arg TORCHVISION_VERSION="${TORCHVISION_VERSION}" \
  --build-arg TORCHAUDIO_VERSION="${TORCHAUDIO_VERSION}" \
  --build-arg CADDY_VERSION="${CADDY_VERSION}" \
  -f Dockerfile .

# --- Final Output ---
echo ""
echo "[SUCCESS] Build complete."
echo "Image ready: ${DOCKER_USERNAME}/${IMAGE_NAME}:${TAG}"
echo "To push this image to Docker Hub, run: docker push ${DOCKER_USERNAME}/${IMAGE_NAME}:${TAG}"


---
File: /Caddyfile
---

# ==============================================================================
# Caddy Webserver Configuration
# ==============================================================================
#
# DESCRIPTION:
# This file configures the Caddy webserver to act as a simple reverse proxy.
# It listens on the container's public port 80 and forwards all traffic
# to the TabbyAPI server running internally on port 5000.
#

{
  # RATIONALE: Disable automatic HTTPS certificate generation. SSL/TLS termination
  # is handled by upstream services (like TailscaleHeadscale, RunPod's proxy or 
  # another load balancer).
  auto_https off
}

# Listen on port 80 for all incoming HTTP requests.
:80 {
  # Forward all requests to the TabbyAPI server.
  reverse_proxy localhost:5000
}


---
File: /config.yml.sample
---

# ==============================================================================
# TabbyAPI Configuration File
# ==============================================================================
#
# DESCRIPTION:
# This file controls the behavior of the TabbyAPI server, including networking,
# logging, and model loading parameters.
#
# AI-NOTE: This file is a primary interface for users to customize the server.
# Key parameters include `host`, `port`, `model_dir`, and `model_name`.
#

# ------------------------------------------------------------------------------
# SECTION: Network
# PURPOSE: Configures the server's host, port, and API-level network settings.
# ------------------------------------------------------------------------------
network:
  host: 0.0.0.0
  port: 5000
  disable_auth: false
  disable_fetch_requests: false
  send_tracebacks: false
  api_servers: ["OAI"]

# ------------------------------------------------------------------------------
# SECTION: Logging
# PURPOSE: Controls the verbosity of server logging. Useful for privacy and
#          performance tuning.
# ------------------------------------------------------------------------------
logging:
  log_prompt: false
  log_generation_params: false
  log_requests: false

# ------------------------------------------------------------------------------
# SECTION: Model
# PURPOSE: Defines the primary model to be loaded and its runtime parameters.
#          This is the most critical section for performance and memory tuning.
# ------------------------------------------------------------------------------
model:
  # PARAMETER: model_dir
  # The base directory where model folders are stored. The server will look
  # for the `model_name` directory inside this path.
  # RATIONALE (RunPod): This is set to `/workspace/models` to utilize the
  # persistent volume, separating large model files from the container image.
  model_dir: /workspace/models

  # PARAMETER: model_name
  # The name of the specific model folder to load, located within `model_dir`.
  model_name: REPLACE_ME

  # PARAMETER: max_seq_len
  # The maximum sequence length (context window) the model will support.
  # NOTE: Higher values consume significantly more VRAM, especially for the KV cache.
  # Ensure this value is compatible with the chosen model's architecture.
  max_seq_len: 65536

  # PARAMETER: rope_scale / rope_alpha
  # These parameters adjust RoPE scaling for models that require it to reach
  # a longer context window than they were trained for.
  # NOTE: Leave commented out to use the model's default values from its config.json.
  # rope_scale:
  # rope_alpha:

  # PARAMETER: use_dummy_models
  # If true, the server will start without loading a real model into VRAM.
  # Useful for testing the API server itself without requiring a GPU.
  use_dummy_models: false
  
  # PARAMETER: cache_mode
  # Determines the data type used for the KV (attention) cache.
  # OPTIONS: "FP16" (full precision), "Q4" (4-bit quantized).
  # RATIONALE: Using "Q4" dramatically reduces the VRAM required for the
  # context window (by ~75% compared to FP16), making large context
  # sizes on VRAM-constrained GPUs feasible. This is the key setting
  # that enables a 32k context on a 48GB card for a 70B model.
  cache_mode: Q4

  # PARAMETER: chunk_size
  # The size of chunks for processing prompts. Can be lowered to reduce VRAM
  # usage during prompt ingestion at a slight cost to performance.
  chunk_size: 2048
  
  # PARAMETER: cache_size
  # The size of the KV cache in tokens.
  # NOTE: By default (commented out), this will be set to `max_seq_len`.
  # cache_size:

  # --- Non-GPU specific settings ---
  inline_model_loading: false
  dummy_model_names: ["gpt-3.5-turbo"]
  use_as_default: []
  tensor_parallel: false
  gpu_split_auto: true
  autosplit_reserve: [96]
  gpu_split: []
  prompt_template:
  vision: false

# ------------------------------------------------------------------------------
# SECTION: Draft Model
# PURPOSE: Configuration for an optional, smaller "draft" model used for
#          speculative decoding to speed up generation. (Advanced feature).
# ------------------------------------------------------------------------------
draft_model:
  draft_model_dir: models
  draft_model_name:
  draft_rope_scale: 1.0
  draft_rope_alpha:
  draft_cache_mode: FP16
  draft_gpu_split: []

# ------------------------------------------------------------------------------
# SECTION: LoRA
# PURPOSE: Configuration for loading LoRA (Low-Rank Adaptation) adapters to
#          modify the behavior of the base model.
# ------------------------------------------------------------------------------
lora:
  lora_dir: loras
  loras:

# ------------------------------------------------------------------------------
# SECTION: Embeddings
# PURPOSE: Configuration for loading a dedicated model for generating text
#          embeddings.
# ------------------------------------------------------------------------------
embeddings:
  embedding_model_dir: models
  embeddings_device: cpu
  embedding_model_name:

# ------------------------------------------------------------------------------
# SECTION: Sampling
# PURPOSE: Allows for overriding default sampling parameters from a preset file.
# ------------------------------------------------------------------------------
sampling:
  override_preset:

# ------------------------------------------------------------------------------
# SECTION: Developer
# PURPOSE: Contains experimental or debugging flags. Use with caution.
# ------------------------------------------------------------------------------
developer:
  unsafe_launch: false
  disable_request_streaming: false
  cuda_malloc_backend: false
  realtime_process_priority: false


---
File: /Dockerfile
---

# syntax=docker/dockerfile:1
# ==============================================================================
# Dockerfile for TabbyAPI
# ==============================================================================
#
# SUMMARY:
# This is a multi-stage Dockerfile designed to create a secure, minimal, and
# reproducible production environment for the TabbyAPI application.
#
# STAGE 0 (builder): Compiles all Python dependencies into a self-contained
# "wheelhouse". This allows the final stage to install everything without
# needing network access, which is faster and more secure.
#
# STAGE 1 (runtime): A minimal image that copies artifacts from the builder
# stage. It sets up the user, application code, and services to run.
#
# ARCHITECTURAL NOTE FOR AI:
# The key design choice here is the separation of the application code from
# persistent data. The application is installed in `/opt/tabbyapi-src`, while
# persistent data (like models) is expected to be in `/workspace`, which is
# mounted as a volume on platforms like RunPod. This avoids the "shadowed
# directory" problem where a volume mount hides the underlying application code.
#
################################################################################
# STAGE 0: The Builder
################################################################################
# AI-NOTE: The ARGs defined here control the versions of all major components.
ARG CUDA_VERSION="12.8.0"
ARG PYTHON_VERSION="3.11"
ARG TORCH_VERSION="2.7.1+cu128"
ARG TORCHVISION_VERSION="0.22.1+cu128"
ARG TORCHAUDIO_VERSION="2.7.1+cu128"

# Start from the NVIDIA CUDA development image, which includes the full CUDA toolkit.
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS builder

# Redeclare ARGs to make them available in this build stage.
ARG TORCH_VERSION
ARG TORCHVISION_VERSION
ARG TORCHAUDIO_VERSION
ARG XFORMERS_VERSION

# Set environment variables for non-interactive setup and CUDA architecture.
ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC \
    TORCH_CUDA_ARCH_LIST="8.6" \
    CUDA_HOME=/usr/local/cuda \
    PATH="/usr/local/cuda/bin:${PATH}"

# Install base system dependencies and a specific Python version from the deadsnakes PPA.
RUN apt-get update && \
    apt-get install -y --no-install-recommends software-properties-common \
                       ca-certificates git ninja-build && \
    add-apt-repository ppa:deadsnakes/ppa && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        python3.11-dev python3.11-venv python3.11-distutils python3-pip && \
    ln -sf /usr/local/cuda-12.8 /usr/local/cuda && \
    ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    python3.11 -m pip install --no-cache-dir --upgrade pip setuptools wheel && \
    rm -rf /var/lib/apt/lists/* && \
    mkdir -p /wheels

# --- Application Source Preparation ---
# Goal: Clone the TabbyAPI repository and apply necessary patches to make it
# compatible with our specific environment (e.g., using exllamav3).
WORKDIR /workspace
RUN git clone --depth 1 --branch main https://github.com/theroyallab/TabbyAPI.git
WORKDIR /workspace/TabbyAPI
RUN set -eux; \
    # OPTIMIZATION: Remove the hidden .git directory after cloning. This directory
    # contains git metadata that is unnecessary for the runtime container and
    # can add several megabytes of bloat to the final image.
    rm -rf .git && \
    # RATIONALE: Patch the source to use exllamav3 instead of the default exllamav2.
    sed -i 's/from backends.exllamav2.model import ExllamaV2Container/from backends.exllamav3.model import ExllamaV3Container/' common/model.py && \
    sed -i 's/new_container = await ExllamaV2Container.create(/new_container = await ExllamaV3Container.create(/' common/model.py && \
    if [ -f backends/exllamav2/version.py ]; then \
        sed -i 's/raise SystemExit(("Exllamav2 is not installed.\\n" + install_message))/pass/' backends/exllamav2/version.py; \
    fi && \
    # RATIONALE: The original pyproject.toml has brittle, hardcoded URLs for its optional
    # dependencies. We remove them entirely to ensure our build is stable and reproducible.
    awk ' \
      BEGIN {inblock=0} \
      /^\[project.optional-dependencies\]/ {inblock=1; next} \
      /^\[/ {if(inblock){inblock=0} } \
      !inblock \
    ' pyproject.toml > pyproject.toml.tmp && mv pyproject.toml.tmp pyproject.toml && \
    # RATIONALE: We add back a clean, dynamically generated [cu128] extra that uses
    # the specific Torch versions defined in our ARGs.
    printf '\n[project.optional-dependencies]\ncu128 = [\n    "torch==%s",\n    "torchvision==%s",\n    "torchaudio==%s",\n]\n' \
        "${TORCH_VERSION}" "${TORCHVISION_VERSION}" "${TORCHAUDIO_VERSION}" >> pyproject.toml

# --- Wheelhouse Creation ---
# Goal: Create a complete, self-contained set of Python wheels for all dependencies.
# This allows for a fully offline installation in the final runtime stage.
RUN --mount=type=cache,target=/root/.cache/pip \
    set -eux; \
    # 1. Install pip-tools for dependency resolution.
    python3.11 -m pip install --no-cache-dir pip-tools; \
    # 2. Define the top-level requirements for the project.
    { \
        echo "torch==${TORCH_VERSION}"; \
        echo "torchvision==${TORCHVISION_VERSION}"; \
        echo "torchaudio==${TORCHAUDIO_VERSION}"; \
		# RATIONALE: xformers is NOT included. The primary reason for its inclusion in
        # the main branch was as a fallback dependency for exllamav2 on pre-Ampere GPUs.
        # This branch uses exllamav3 (dev), which does not support pre-Ampere GPUs,
        # making the xformers fallback unnecessary.
        echo "exllamav3 @ git+https://github.com/turboderp-org/exllamav3.git@dev"; \
        echo "/workspace/TabbyAPI"; \
    } > /tmp/requirements.in && \
    # 3. Pre-install torch to satisfy dependencies for other packages during resolution.
    python3.11 -m pip install --no-cache-dir \
        --extra-index-url https://download.pytorch.org/whl/cu128 \
        "torch==${TORCH_VERSION}" && \
    # 4. Use pip-compile to generate a fully-pinned requirements.txt file.
    python3.11 -m piptools compile \
        --extra-index-url https://download.pytorch.org/whl/cu128 \
        --output-file /tmp/requirements.txt \
        /tmp/requirements.in && \
    # 5. Download and build all dependencies into wheels and store them in /wheels.
    python3.11 -m pip wheel --wheel-dir=/wheels \
         --extra-index-url https://download.pytorch.org/whl/cu128 \
         -r /tmp/requirements.txt && \
    # 6. Clean up the pre-installed packages from the builder stage.
    python3.11 -m pip uninstall -y torch torchvision torchaudio

################################################################################
# STAGE 1: The Runtime
################################################################################
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu22.04

# Redeclare ARGs needed in this stage.
ARG CADDY_VERSION="2.7.6"

# Set environment variables.
ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC

# OPTIMIZATION: Consolidate all OS-level setup and cleanup into a single RUN command.
# RATIONALE: Each RUN command creates a new layer in the Docker image. By combining
# these steps, we reduce the number of layers. More importantly, by running
# `rm -rf /var/lib/apt/lists/*` in the *same* layer that `apt-get install` was
# used, we ensure the apt cache files are truly discarded from the final image,
# significantly reducing its size.
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        software-properties-common iproute2 curl gnupg ca-certificates \
        dos2unix gosu libcap2-bin && \
    add-apt-repository ppa:deadsnakes/ppa && \
    apt-get update && \
    apt-get install -y --no-install-recommends python3.11 && \
    curl -sS https://bootstrap.pypa.io/get-pip.py | python3.11 && \
    ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg \
         | gpg --dearmor -o /usr/share/keyrings/tailscale-archive-keyring.gpg && \
    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list \
         | tee /etc/apt/sources.list.d/tailscale.list && \
    apt-get update && apt-get install -y --no-install-recommends tailscale && \
    # Create a non-root user for security.
    groupadd --gid 1000 somneruser && \
    useradd --uid 1000 --gid 1000 --shell /bin/bash --create-home somneruser && \
    # Clean up apt cache to minimize image size.
    rm -rf /var/lib/apt/lists/*

# Copy build artifacts from the builder stage.
COPY --from=builder /wheels /wheels
# RATIONALE: This copies the application source code to a safe, non-volume location (/opt).
# This prevents the app code from being hidden by a volume mounted at /workspace on RunPod.
# OPTIMIZATION: The source of this copy is smaller because the .git directory was removed in the builder stage.
COPY --from=builder /workspace/TabbyAPI /opt/tabbyapi-src

# OPTIMIZATION: Consolidate all Python package installation and cleanup into a single RUN command.
# RATIONALE: Similar to the apt-get optimization, this creates a single layer. We install all
# Python packages from the `/wheels` directory and then delete that directory in the same command.
# This ensures the large wheel files (~10-15GB) are not part of the final image.
RUN \
    set -eux; \
    # STEP 1: Install all heavyweight dependencies from the pre-built wheels.
    # The --no-cache-dir flag prevents pip from storing a cache, and --no-index
    # ensures it only uses the local /wheels directory.
    python3.11 -m pip install --no-cache-dir --no-index --find-links=/wheels \
        torch torchvision torchaudio exllamav3 && \
    \
    # STEP 2: Install TabbyAPI itself from its source directory in /opt.
    python3.11 -m pip install --no-cache-dir /opt/tabbyapi-src[cu128] && \
    \
    # STEP 3: Grant ownership of the app directory to the runtime user for logs/etc.
    chown -R somneruser:somneruser /opt/tabbyapi-src && \
    \
    # STEP 4: Clean up the wheelhouse and any pycache files to keep the final image small.
    rm -rf /wheels && \
    find /usr/local/lib/python3.11/ -name "__pycache__" -type d -exec rm -rf {} +
	
# Configure the application environment by copying in config files.
# The WORKDIR is set first to ensure the files are copied to the correct location.
WORKDIR /opt/tabbyapi-src
COPY Caddyfile /etc/caddy/Caddyfile
COPY config.yml api_tokens.yml ./
COPY start_services.sh /usr/local/bin/start_services.sh
RUN dos2unix /usr/local/bin/start_services.sh && chmod +x /usr/local/bin/start_services.sh

# Install and configure Caddy webserver.
RUN curl -fsSL https://github.com/caddyserver/caddy/releases/download/v${CADDY_VERSION}/caddy_${CADDY_VERSION}_linux_amd64.tar.gz \
        | tar -xz -C /usr/local/bin caddy && \
    setcap 'cap_net_bind_service=+ep' /usr/local/bin/caddy

# --- Final Runtime Configuration ---
# Switch to the non-root user for security.
USER somneruser
# Set the final working directory for the running application.
WORKDIR /opt/tabbyapi-src
# Set standard XDG environment variables.
ENV XDG_CONFIG_HOME=/home/somneruser/.config \
    XDG_DATA_HOME=/home/somneruser/.local/share

# Define runtime behavior
HEALTHCHECK --interval=1m --timeout=15s --start-period=10m --retries=3 \
  CMD curl --fail http://127.0.0.1:5000/health || exit 1

EXPOSE 80
ENTRYPOINT ["/usr/local/bin/start_services.sh"]


---
File: /README.md
---

> **⚠️ WARNING: You are on the `dev/experimental-backend` development branch.**
>
> This branch contains experimental changes and may be unstable. It uses the development branch of **ExllamaV3**, which has different hardware requirements than the stable `main` branch.
>
> **CRITICAL: ExllamaV3 requires an NVIDIA GPU with Ampere architecture or newer (e.g., RTX 30xx, RTX 40xx, A4000+, A100). Pre-Ampere GPUs (e.g., Turing, Volta, Tesla T4, RTX 20xx) are NOT supported on this branch.**
>
> For stable, broader hardware support, please use the `main` branch.

===========================================

# TabbyAPI-Somner

A production-ready containerized deployment of [TabbyAPI](https://github.com/theroyallab/TabbyAPI) optimized for secure, remote local or air-gapped environments with [ExllamaV3](https://github.com/turboderp-org/exllamav3) acceleration and mesh networking capabilities.

This project is defined by its main [Dockerfile](./Dockerfile) and started by the [start_services.sh](./start_services.sh) script.

## 🚀 Modern Technology Stack

- **[TabbyAPI](https://github.com/theroyallab/TabbyAPI)** - Enhanced with [ExllamaV3 (dev branch)](https://github.com/turboderp-org/exllamav3) inference backend for optimal throughput
- **[CUDA 12.8.0](https://developer.nvidia.com/cuda-downloads)** - Latest [NVIDIA](https://www.nvidia.com/) driver compatibility with optimized binaries
- **[PyTorch 2.7.1+cu128](https://pytorch.org/)** - Current [PyTorch](https://pytorch.org/) ecosystem with CUDA 12.8 acceleration
- **[Flash Attention](https://github.com/Dao-AILab/flash-attention)** - Memory-efficient attention mechanism (included as a dependency of ExllamaV3)
- **[Python 3.11](https://www.python.org/)** - Modern Python runtime with performance improvements
- **[Caddy 2.7.6](https://caddyserver.com/)** - Zero-config HTTPS reverse proxy using this [Caddyfile](./Caddyfile).
- **[Tailscale](https://tailscale.com/)** - Built-in mesh networking for secure remote access

## 🔒 Privacy-First Security Design

- **No Request Logging** - Disabled prompt logging, generation params, and request history
- **No Traceback Exposure** - Server errors don't leak internal information
- **Reduced Attack Surface** - Minimal dependencies, non-root execution, capability-based security
- **Air-Gap Ready** - Pre-built dependency wheelhouse eliminates external network requirements
- **Mesh Network Security** - Encrypted [Tailscale](https://tailscale.com/) tunnels without port forwarding

---

## 📦 Getting Started Guide

### Step 1: Prerequisites

1.  **Install [Tailscale](https://tailscale.com/)** on your client device.
2.  **Get your Tailscale auth key** from [https://login.tailscale.com/admin/settings/keys](https://login.tailscale.com/admin/settings/keys).

### Step 2: Deploy with Docker

You can build the image yourself using the provided [`build.sh.sample`](./build.sh.sample) script, or pull a pre-built image.

```bash
# Pull the development image
# Replace 'yourusername/somner:dev1' with your own image if you built it yourself
docker pull yourusername/somner:dev1

# On your host machine, create a directory for your models
mkdir -p ./models

# Run the container (see Step 3 below before running)
docker run -d \
  --name tabbyapi-somner-dev \
  --gpus all \
  -v ./models:/workspace/models \
  -e TAILSCALE_AUTHKEY=your-auth-key-here \
  -p 80:80 \
  yourusername/somner:dev1
```

### Step 3: First-Time Configuration (Setting Your Model)

When you first launch the container, it may fail to start with a "model not found" or "config file not found" error in the logs. **This is expected.** The container needs to be told which model you want to use from your persistent `/workspace` volume.

You must create a `config.yml` file at the root of your `/workspace` volume.

**1. Connect to Your Volume:**
Open a terminal to your RunPod volume (or use `docker exec -it <container_name> /bin/bash` if running locally).

**2. Copy the Sample Configuration:**
The container includes a sample config file, [`config_sample.yml`](./config_sample.yml). Run this command to copy it to the correct location on your persistent volume:

```bash
cp /opt/tabbyapi-src/config_sample.yml /workspace/config.yml
```

**3. Edit Your New `config.yml`:**
Open the file you just created and set the `model_name` to match the directory of the model you have downloaded.

```bash
# Open the file for editing
nano /workspace/config.yml
```

**Example:**
```yaml
model:
  model_dir: /workspace/models
  # Change this to your model's folder name
  model_name: L3.3-YOUR-MODEL-HERE 
```

**4. Restart the Pod:**
Save your changes and restart the pod. The startup script will now automatically find and use `/workspace/config.yml`. No extra volume mounts are needed for the config file if you are on a platform like RunPod that mounts the entire `/workspace`.

#### Why This Approach?
This method separates **configuration** (your settings) from the immutable **container**. To swap models, you can now edit your `/workspace/config.yml` (created from [`config_sample.yml`](./config_sample.yml)) and restart your session/pod without ever needing to rebuild the container image.

---

## 🔧 Advanced Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `TAILSCALE_AUTHKEY` | [Tailscale](https://tailscale.com/) authentication key | Required |

### Local Docker Configuration Example

For local Docker runs (not on RunPod), you must map your host files and directories into the container's `/workspace`. The container's internal logic remains the same.

```bash
# Create the necessary files and directories on your local machine first.
# You will need a config.yml and api_tokens.yml. You can create these
# from their respective samples: config_sample.yml and api_tokens.yml.sample
mkdir -p ./models
touch ./config.yml
touch ./api_tokens.yml

# Run the container, mapping all necessary files
docker run -d \
  --name tabbyapi-somner-dev \
  --gpus all \
  -v "$(pwd)/models":/workspace/models \
  -v "$(pwd)/config.yml":/workspace/config.yml \
  -v "$(pwd)/api_tokens.yml":/workspace/api_tokens.yml \
  -e TAILSCALE_AUTHKEY=your-auth-key-here \
  yourusername/somner:dev1
```
*Note: We are mapping to `/workspace/config.yml`, not `/opt/tabbyapi-src/config.yml`.*

### Model Customization
To customize model behavior, edit your persistent [`/workspace/config.yml`](./config_sample.yml) file. You can change:
- Maximum sequence length
- Cache settings (`cache_mode`)
- GPU memory allocation (`gpu_split_auto`)
- Sampling parameters

---

## 🔒 Network Configuration

### Tailscale ACLs

To allow your devices to securely connect to the container, you must configure your Tailscale network's Access Control Lists (ACLs). This project includes a recommended sample file to make this easy.

**One-Time Setup:**

1.  **Find the Sample File:** In this repository, locate the file named [`tailscale-acl.json.sample`](./tailscale-acl.json.sample).
2.  **Edit and Apply:** Follow the instructions in the sample file to apply the ACLs to your Tailscale admin console. This only needs to be done once.

### API Access

Once running, the container provides:
- **Local Access**: `http://localhost:80`
- **Mesh Network Access**: `http://<containers-tailscale-ip>:80/v1` (via [Tailscale](https://tailscale.com/))
- **[OpenAI-Compatible API](https://platform.openai.com/docs/api-reference)**: Drop-in replacement for [OpenAI API](https://platform.openai.com/docs/api-reference) endpoints

---

## 📊 Performance Features

- **[ExllamaV3](https://github.com/turboderp-org/exllamav3) Backend** - Optimized inference performance
- **[Flash Attention](https://github.com/Dao-AILab/flash-attention)** - Memory-efficient attention computation
- **Automatic GPU Splitting** - Multi-GPU support with automatic memory allocation
- **FP16 Caching** - Reduced memory footprint with maintained precision
- **Tensor Parallelism** - Distributed computation across multiple GPUs

## 🛡️ Security Features

- **Non-Root Execution** - All services run as unprivileged user
- **Capability-Based Security** - Minimal system capabilities
- **No External Dependencies** - All packages pre-built during image creation
- **Encrypted Mesh Networking** - [Tailscale](https://tailscale.com/) [WireGuard](https://www.wireguard.com/) tunnels
- **Request Isolation** - No persistent request logging or history

## 📋 System Requirements

- **GPU**: NVIDIA GPU with **Ampere architecture or newer** is required.
  - **Supported Architectures**: Ampere, Hopper (e.g., RTX 30-series, RTX 40-series, A100, H100).
  - **Unsupported Architectures**: Turing, Volta, Pascal (e.g., RTX 20-series, Titan V, Tesla T4, V100, P100).
  - **AI-NOTE**: This is a key difference from the `main` branch. The `main` branch uses ExllamaV2, which can fall back to using `xformers` for compatibility with pre-Ampere GPUs. ExllamaV3 (used in this branch) drops this backward compatibility in favor of performance on modern hardware and therefore does not support pre-Ampere GPUs.
- **Memory**: 8GB+ system RAM, 16GB+ VRAM recommended for 70B models.
- **Storage**: 10GB+ for container, plus additional space for models.
- **Network**: [Tailscale](https://tailscale.com/) account for optional mesh networking.

## 🔍 Health Monitoring

The container includes built-in health checks:
- API endpoint availability
- Service process monitoring
- GPU memory status
- Network connectivity

## 📄 License

This project builds upon [TabbyAPI](https://github.com/theroyallab/TabbyAPI) and related open-source projects. Please respect the licenses of all included components.

## 🤝 Contributing

Contributions are welcome! Please submit issues and pull requests for:
- Performance optimizations
- Security enhancements
- Documentation improvements



---
File: /start_services.sh
---

#!/usr/bin/env bash
# ==============================================================================
# Service Supervisor for TabbyAPI Container
# ==============================================================================
#
# DESCRIPTION:
# This script is the designated ENTRYPOINT for the container. Its primary
# responsibilities are:
#   1. Performing initial, one-time setup as the root user.
#   2. Dropping privileges to a non-root user ('somneruser') for enhanced security.
#   3. Starting and supervising all necessary background services (tailscaled,
#      TabbyAPI, Caddy).
#   4. Ensuring a clean shutdown if any supervised service fails.
#
# AI-NOTE: This script follows a "supervisor" pattern. It launches several
# background processes and uses `wait -n` to pause until one of them exits,
# at which point it terminates the container. This is a lightweight alternative
# to more complex init systems like systemd.
#
# ==============================================================================

# ------------------------------------------------------------------------------
# Script Configuration and Safety
# ------------------------------------------------------------------------------
# `set -e`: Exit immediately if a command exits with a non-zero status.
# `set -u`: Treat unset variables as an error when substituting.
# `set -o pipefail`: The return value of a pipeline is the status of the last
#                    command to exit with a non-zero status, or zero if no
#                    command exited with a non-zero status.
set -euo pipefail

# ==============================================================================
# Initial Root-Level Setup & Privilege Drop
# ==============================================================================
# RATIONALE: The script starts as the `root` user. This block performs actions
# that require root privileges (like creating state directories) before
# re-executing itself as the unprivileged `somneruser` using `gosu`.
# This is a critical security best practice. All subsequent commands will run
# as the non-root user.
# ------------------------------------------------------------------------------
if [ "$(id -u)" -eq 0 ]; then
    # Create state and socket directories for Tailscale, owned by the runtime user.
    TS_STATE_DIR="/home/somneruser/.local/state/tailscale"
    TS_SOCKET_DIR="/home/somneruser/.local/run/tailscale"
    mkdir -p "$TS_STATE_DIR" "$TS_SOCKET_DIR"
    chown -R somneruser:somneruser "$TS_STATE_DIR" "$TS_SOCKET_DIR"
    # Re-execute this script as 'somneruser'
    exec gosu somneruser "$0" "$@"
fi

# ==============================================================================
# --- From this point on, the script is running as the non-root 'somneruser' ---
# ==============================================================================

# ==============================================================================
# Enhanced Runtime Diagnostics
# ==============================================================================
# RATIONALE: This block provides a quick "health check" of the Python environment
# upon container startup. It verifies that key dependencies can be imported,
# allowing for fast failure detection if the environment is misconfigured.
# It runs *before* the main services to provide clear, early diagnostic output.
# ------------------------------------------------------------------------------
echo ">>> Final Import Test:"
python3.11 -c "import torch, flash_attn, exllamav3; print('✅ All key DEPENDENCIES imported successfully!')" || { echo "❌ Dependency import test failed!"; exit 1; }
echo "=== End Diagnostics ==="

# ==============================================================================
# Service Definitions and Startup
# ==============================================================================
# Define state directory paths for user-level services.
TS_STATE_DIR="/home/somneruser/.local/state/tailscale"
TS_SOCKET_DIR="/home/somneruser/.local/run/tailscale"
# Allow TAILSCALE_AUTHKEY to be passed in as an environment variable, but don't fail if it's not set.
: "${TAILSCALE_AUTHKEY:=}"

# 1. Start Tailscale Daemon
# The tailscaled process is started in the background (&).
echo "[INFO] Starting tailscaled..." >&2
tailscaled \
  --state="${TS_STATE_DIR}/tailscaled.state" \
  --socket="${TS_SOCKET_DIR}/tailscaled.sock" \
  --tun=userspace-networking &
TAILSCALED_PID=$!
# Give the daemon a moment to initialize.
sleep 5

# 2. Initialize Tailscale Interface
# Connects the container to the Tailscale network using the provided auth key if available.
echo "[INFO] Bringing up Tailscale interface..." >&2
tailscale --socket="${TS_SOCKET_DIR}/tailscaled.sock" up \
  --hostname="runpod-forge" \
  --accept-dns=false \
  ${TAILSCALE_AUTHKEY:+--auth-key=${TAILSCALE_AUTHKEY}}

# 3. Launch TabbyAPI Server
# ARCHITECTURAL NOTE: We launch the server by executing `main.py` directly.
# This project is structured as a script-based application, not a standard
# installable Python package, so `python -m tabbyapi.main` will not work.
echo "[INFO] Starting TabbyAPI server..." >&2

# RATIONALE: Define the path to the user's persistent config file.
# This decouples the container from its configuration.
CONFIG_PATH="/workspace/config.yml"

# RATIONALE: Add a pre-flight check to ensure the config file exists.
# If it doesn't, fail fast with a clear, actionable error message.
if [ ! -f "$CONFIG_PATH" ]; then
    echo "[FATAL] Configuration file not found at '$CONFIG_PATH'."
    echo "[INFO]  Please follow the 'First-Time Configuration' steps in the README."
    echo "[INFO]  You must create a config.yml on your /workspace volume."
    exit 1
fi

echo "[INFO] Using configuration from persistent volume: $CONFIG_PATH"

# Explicitly change to the source directory to ensure the script's working
# directory is correct, allowing it to find its modules.
cd /opt/tabbyapi-src

# --- Pre-Launch Sanity Check ---
# Log the context right before launching the Python script to aid in debugging.
echo "[SANITY CHECK] Current user is: $(whoami)"
echo "[SANITY CHECK] Current directory is: $(pwd)"
echo "[SANITY CHECK] Contents of current directory:"
ls -la
echo "[SANITY CHECK] PATH variable is: $PATH"
echo "--- End Sanity Check ---"

# Launch the server, pointing it explicitly to the config file on the volume.
python3.11 main.py --config "$CONFIG_PATH" &
TABBY_PID=$!

# 4. Health Check Loop
# RATIONALE: This loop prevents the script from proceeding until the TabbyAPI
# server is actually listening on its port. It also provides an early exit if
# the server process crashes immediately upon startup.
echo "[INFO] Waiting for TabbyAPI to become healthy on port 5000..." >&2
for i in {1..120}; do
  if ss -ltn | grep -q ':5000'; then
    echo "[INFO] TabbyAPI is listening on :5000 (PID $TABBY_PID)." >&2
    break
  fi
  if ! ps -p $TABBY_PID &>/dev/null; then
    echo "[ERROR] TabbyAPI (PID $TABBY_PID) crashed during startup." >&2
    exit 1
  fi
  sleep 1
done

# Fail fatally if the server didn't start after the timeout.
if ! ss -ltn | grep -q ':5000'; then
  echo "[FATAL] TabbyAPI failed to start within 30 seconds." >&2
  exit 1
fi

# 5. Launch Caddy Reverse Proxy
# Starts the Caddy server in the background to act as a reverse proxy.
echo "[INFO] Starting Caddy reverse proxy..." >&2
caddy run --config /etc/caddy/Caddyfile &
CADDY_PID=$!

echo "[INFO] All services ready. PIDs => tailscaled:${TAILSCALED_PID}, tabby:${TABBY_PID}, caddy:${CADDY_PID}" >&2

# 6. Process Supervision
# `wait -n` will pause the script until ANY of the background jobs exit.
# This keeps the container alive and ensures a clean shutdown if any service fails.
wait -n $TAILSCALED_PID $TABBY_PID $CADDY_PID
echo "[INFO] A supervised process has exited. Shutting down."



---
File: /tailscale-acl.json.sample
---

{
	// ==============================================================================
	// Tailscale ACL Policy for TabbyAPI Deployment
	// ==============================================================================
	//
	// INSTRUCTIONS:
	// 1. Open your Tailscale Admin Console -> ACLs.
	// 2. Delete the entire contents of the editor.
	// 3. Paste the contents of this file into the editor.
	// 4. In the "tagOwners" section below, replace "autogroup:admin" with your
	//    own Tailscale login email if you prefer, for example: ["user@example.com"].
	// 5. Save the changes.
	//
	// ==============================================================================

	"tagOwners": {
		// This section defines who is allowed to create devices with specific tags.
		// RATIONALE: Using "autogroup:admin" is a best practice, as it means any user
		// you designate as an Admin in Tailscale can manage these servers.
		"tag:runpod-forge-servers": ["autogroup:admin"]
	},

	"grants": [
		// Grant #1: Allow access to the RunPod servers on port 80 (HTTP).
		// This rule is the pinhole that allows your client (e.g., SillyTavern)
		// to connect to the Caddy reverse proxy inside the container.
		{
			"src": ["autogroup:member"],
			"dst": ["tag:runpod-forge-servers"],
			"ip":  ["80"]
		},

		// Grant #2: Allow general communication between all your personal devices.
		// This is a standard rule that makes your private network feel like a
		// normal LAN, allowing you to SSH, share files, etc., between your clients.
		{
			"src": ["autogroup:member"],
			"dst": ["autogroup:member"],
			"ip":  ["*"]
		}
	],

	"ssh": [
		// This is a standard default that allows any user to SSH into
		// any of their own devices as either a regular user or root.
		{
			"action": "check",
			"src":    ["autogroup:member"],
			"dst":    ["autogroup:self"],
			"users":  ["autogroup:nonroot", "root"]
		}
	]
}

