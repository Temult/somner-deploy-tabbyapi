Directory Structure:

└── ./
    ├── api_tokens.yml.sample
    ├── Caddyfile
    ├── config.yml
    ├── Dockerfile
    ├── README.md
    └── start_services.sh



---
File: /api_tokens.yml.sample
---

api_key: REPLACE_ME
admin_key: REPLACE_ME


---
File: /Caddyfile
---

# ==============================================================================
# Caddy Webserver Configuration
# ==============================================================================
#
# DESCRIPTION:
# This file configures the Caddy webserver to act as a simple reverse proxy.
# It listens on the container's public port 80 and forwards all traffic
# to the TabbyAPI server running internally on port 5000.
#

{
  # RATIONALE: Disable automatic HTTPS certificate generation. SSL/TLS termination
  # is handled by upstream services (like TailscaleHeadscale, RunPod's proxy or 
  # another load balancer).
  auto_https off
}

# Listen on port 80 for all incoming HTTP requests.
:80 {
  # Forward all requests to the TabbyAPI server.
  reverse_proxy localhost:5000
}


---
File: /config.yml
---

# ==============================================================================
# TabbyAPI Configuration File
# ==============================================================================
#
# DESCRIPTION:
# This file controls the behavior of the TabbyAPI server, including networking,
# logging, and model loading parameters.
#
# AI-NOTE: This file is a primary interface for users to customize the server.
# Key parameters include `host`, `port`, `model_dir`, and `model_name`.
#

# ------------------------------------------------------------------------------
# SECTION: Network
# PURPOSE: Configures the server's host, port, and API-level network settings.
# ------------------------------------------------------------------------------
network:
  host: 0.0.0.0
  port: 5000
  disable_auth: false
  disable_fetch_requests: false
  send_tracebacks: false
  api_servers: ["OAI"]

# ------------------------------------------------------------------------------
# SECTION: Logging
# PURPOSE: Controls the verbosity of server logging. Useful for privacy and
#          performance tuning.
# ------------------------------------------------------------------------------
logging:
  log_prompt: false
  log_generation_params: false
  log_requests: false

# ------------------------------------------------------------------------------
# SECTION: Model
# PURPOSE: Defines the primary model to be loaded and its runtime parameters.
#          This is the most critical section for performance and memory tuning.
# ------------------------------------------------------------------------------
model:
  # PARAMETER: model_dir
  # The base directory where model folders are stored. The server will look
  # for the `model_name` directory inside this path.
  # RATIONALE (RunPod): This is set to `/workspace/models` to utilize the
  # persistent volume, separating large model files from the container image.
  model_dir: /workspace/models

  # PARAMETER: model_name
  # The name of the specific model folder to load, located within `model_dir`.
  model_name: Doctor-Shotgun_L3.3-70B-Magnum-Diamond-EXL3

  # PARAMETER: max_seq_len
  # The maximum sequence length (context window) the model will support.
  # NOTE: Higher values consume significantly more VRAM, especially for the KV cache.
  # Ensure this value is compatible with the chosen model's architecture.
  max_seq_len: 32768

  # PARAMETER: rope_scale / rope_alpha
  # These parameters adjust RoPE scaling for models that require it to reach
  # a longer context window than they were trained for.
  # NOTE: Leave commented out to use the model's default values from its config.json.
  # rope_scale:
  # rope_alpha:

  # PARAMETER: use_dummy_models
  # If true, the server will start without loading a real model into VRAM.
  # Useful for testing the API server itself without requiring a GPU.
  use_dummy_models: false
  
  # PARAMETER: cache_mode
  # Determines the data type used for the KV (attention) cache.
  # OPTIONS: "FP16" (full precision), "Q4" (4-bit quantized).
  # RATIONALE: Using "Q4" dramatically reduces the VRAM required for the
  # context window (by ~75% compared to FP16), making large context
  # sizes on VRAM-constrained GPUs feasible. This is the key setting
  # that enables a 32k context on a 48GB card for a 70B model.
  cache_mode: Q4

  # PARAMETER: chunk_size
  # The size of chunks for processing prompts. Can be lowered to reduce VRAM
  # usage during prompt ingestion at a slight cost to performance.
  chunk_size: 2048
  
  # PARAMETER: cache_size
  # The size of the KV cache in tokens.
  # NOTE: By default (commented out), this will be set to `max_seq_len`.
  # cache_size:

  # --- Non-GPU specific settings ---
  inline_model_loading: false
  dummy_model_names: ["gpt-3.5-turbo"]
  use_as_default: []
  tensor_parallel: false
  gpu_split_auto: true
  autosplit_reserve: [96]
  gpu_split: []
  prompt_template:
  vision: false

# ------------------------------------------------------------------------------
# SECTION: Draft Model
# PURPOSE: Configuration for an optional, smaller "draft" model used for
#          speculative decoding to speed up generation. (Advanced feature).
# ------------------------------------------------------------------------------
draft_model:
  draft_model_dir: models
  draft_model_name:
  draft_rope_scale: 1.0
  draft_rope_alpha:
  draft_cache_mode: FP16
  draft_gpu_split: []

# ------------------------------------------------------------------------------
# SECTION: LoRA
# PURPOSE: Configuration for loading LoRA (Low-Rank Adaptation) adapters to
#          modify the behavior of the base model.
# ------------------------------------------------------------------------------
lora:
  lora_dir: loras
  loras:

# ------------------------------------------------------------------------------
# SECTION: Embeddings
# PURPOSE: Configuration for loading a dedicated model for generating text
#          embeddings.
# ------------------------------------------------------------------------------
embeddings:
  embedding_model_dir: models
  embeddings_device: cpu
  embedding_model_name:

# ------------------------------------------------------------------------------
# SECTION: Sampling
# PURPOSE: Allows for overriding default sampling parameters from a preset file.
# ------------------------------------------------------------------------------
sampling:
  override_preset:

# ------------------------------------------------------------------------------
# SECTION: Developer
# PURPOSE: Contains experimental or debugging flags. Use with caution.
# ------------------------------------------------------------------------------
developer:
  unsafe_launch: false
  disable_request_streaming: false
  cuda_malloc_backend: false
  realtime_process_priority: false


---
File: /Dockerfile
---

# syntax=docker/dockerfile:1
# ==============================================================================
# Dockerfile for TabbyAPI
# ==============================================================================
#
# SUMMARY:
# This is a multi-stage Dockerfile designed to create a secure, minimal, and
# reproducible production environment for the TabbyAPI application.
#
# STAGE 0 (builder): Compiles all Python dependencies into a self-contained
# "wheelhouse". This allows the final stage to install everything without
# needing network access, which is faster and more secure.
#
# STAGE 1 (runtime): A minimal image that copies artifacts from the builder
# stage. It sets up the user, application code, and services to run.
#
# ARCHITECTURAL NOTE FOR AI:
# The key design choice here is the separation of the application code from
# persistent data. The application is installed in `/opt/tabbyapi-src`, while
# persistent data (like models) is expected to be in `/workspace`, which is
# mounted as a volume on platforms like RunPod. This avoids the "shadowed
# directory" problem where a volume mount hides the underlying application code.
#
################################################################################
# STAGE 0: The Builder
################################################################################
# AI-NOTE: The ARGs defined here control the versions of all major components.
ARG CUDA_VERSION="12.8.0"
ARG PYTHON_VERSION="3.11"
ARG TORCH_VERSION="2.7.1+cu128"
ARG TORCHVISION_VERSION="0.22.1+cu128"
ARG TORCHAUDIO_VERSION="2.7.1+cu128"
ARG FLASH_VERSION="2.8.0.post2"

# Start from the NVIDIA CUDA development image, which includes the full CUDA toolkit.
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS builder

# Redeclare ARGs to make them available in this build stage.
ARG TORCH_VERSION
ARG TORCHVISION_VERSION
ARG TORCHAUDIO_VERSION
ARG FLASH_VERSION

# Set environment variables for non-interactive setup and CUDA architecture.
ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC \
    TORCH_CUDA_ARCH_LIST="8.6" \
    CUDA_HOME=/usr/local/cuda \
    PATH="/usr/local/cuda/bin:${PATH}"

# Install base system dependencies and a specific Python version from the deadsnakes PPA.
RUN apt-get update && \
    apt-get install -y --no-install-recommends software-properties-common \
                       ca-certificates git ninja-build && \
    add-apt-repository ppa:deadsnakes/ppa && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        python3.11-dev python3.11-venv python3.11-distutils python3-pip && \
    ln -sf /usr/local/cuda-12.8 /usr/local/cuda && \
    ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    python3.11 -m pip install --no-cache-dir --upgrade pip setuptools wheel && \
    rm -rf /var/lib/apt/lists/* && \
    mkdir -p /wheels

# --- Application Source Preparation ---
# Goal: Clone the TabbyAPI repository and apply necessary patches to make it
# compatible with our specific environment (e.g., using exllamav3).
WORKDIR /workspace
RUN git clone --depth 1 --branch main https://github.com/theroyallab/TabbyAPI.git
WORKDIR /workspace/TabbyAPI
RUN set -eux; \
    # RATIONALE: Patch the source to use exllamav3 instead of the default exllamav2.
    sed -i 's/from backends.exllamav2.model import ExllamaV2Container/from backends.exllamav3.model import ExllamaV3Container/' common/model.py && \
    sed -i 's/new_container = await ExllamaV2Container.create(/new_container = await ExllamaV3Container.create(/' common/model.py && \
    if [ -f backends/exllamav2/version.py ]; then \
        sed -i 's/raise SystemExit(("Exllamav2 is not installed.\\n" + install_message))/pass/' backends/exllamav2/version.py; \
    fi && \
    # RATIONALE: The original pyproject.toml has brittle, hardcoded URLs for its optional
    # dependencies. We remove them entirely to ensure our build is stable and reproducible.
    awk ' \
      BEGIN {inblock=0} \
      /^\[project.optional-dependencies\]/ {inblock=1; next} \
      /^\[/ {if(inblock){inblock=0} } \
      !inblock \
    ' pyproject.toml > pyproject.toml.tmp && mv pyproject.toml.tmp pyproject.toml && \
    # RATIONALE: We add back a clean, dynamically generated [cu128] extra that uses
    # the specific Torch versions defined in our ARGs.
    printf '\n[project.optional-dependencies]\ncu128 = [\n    "torch==%s",\n    "torchvision==%s",\n    "torchaudio==%s",\n]\n' \
        "${TORCH_VERSION}" "${TORCHVISION_VERSION}" "${TORCHAUDIO_VERSION}" >> pyproject.toml

# --- Wheelhouse Creation ---
# Goal: Create a complete, self-contained set of Python wheels for all dependencies.
# This allows for a fully offline installation in the final runtime stage.
RUN --mount=type=cache,target=/root/.cache/pip \
    set -eux; \
    # 1. Install pip-tools for dependency resolution.
    python3.11 -m pip install --no-cache-dir pip-tools; \
    # 2. Define the top-level requirements for the project.
    { \
        echo "torch==${TORCH_VERSION}"; \
        echo "torchvision==${TORCHVISION_VERSION}"; \
        echo "torchaudio==${TORCHAUDIO_VERSION}"; \
        echo "flash-attn==${FLASH_VERSION}"; \
        echo "exllamav3 @ git+https://github.com/turboderp-org/exllamav3.git"; \
        echo "/workspace/TabbyAPI"; \
    } > /tmp/requirements.in && \
    # 3. Pre-install torch to satisfy dependencies for other packages during resolution.
    python3.11 -m pip install --no-cache-dir \
        --extra-index-url https://download.pytorch.org/whl/cu128 \
        "torch==${TORCH_VERSION}" && \
    # 4. Use pip-compile to generate a fully-pinned requirements.txt file.
    python3.11 -m piptools compile \
        --extra-index-url https://download.pytorch.org/whl/cu128 \
        --output-file /tmp/requirements.txt \
        /tmp/requirements.in && \
    # 5. Download and build all dependencies into wheels and store them in /wheels.
    python3.11 -m pip wheel --wheel-dir=/wheels \
         --extra-index-url https://download.pytorch.org/whl/cu128 \
         -r /tmp/requirements.txt && \
    # 6. Clean up the pre-installed packages from the builder stage.
    python3.11 -m pip uninstall -y torch torchvision torchaudio

################################################################################
# STAGE 1: The Runtime
################################################################################
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu22.04

# Redeclare ARGs needed in this stage.
ARG CADDY_VERSION="2.7.6"

# Set environment variables.
ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC

# Install minimal OS packages, Python, Tailscale, and create the non-root user.
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        software-properties-common iproute2 curl gnupg ca-certificates \
        dos2unix gosu libcap2-bin && \
    add-apt-repository ppa:deadsnakes/ppa && \
    apt-get update && \
    apt-get install -y --no-install-recommends python3.11 && \
    curl -sS https://bootstrap.pypa.io/get-pip.py | python3.11 && \
    ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg \
         | gpg --dearmor -o /usr/share/keyrings/tailscale-archive-keyring.gpg && \
    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list \
         | tee /etc/apt/sources.list.d/tailscale.list && \
    apt-get update && apt-get install -y --no-install-recommends tailscale && \
    # Create a non-root user for security.
    groupadd --gid 1000 somneruser && \
    useradd --uid 1000 --gid 1000 --shell /bin/bash --create-home somneruser && \
    rm -rf /var/lib/apt/lists/*

# Copy build artifacts from the builder stage.
COPY --from=builder /wheels /wheels
COPY --from=builder /workspace/TabbyAPI /workspace/TabbyAPI

# Install the application and its dependencies correctly.
RUN \
    set -eux; \
    # STEP 1: Move the application source code to a safe, non-volume location (/opt).
    # RATIONALE: This prevents the app code from being hidden by a volume mounted at /workspace on RunPod.
    mv /workspace/TabbyAPI /opt/tabbyapi-src && \
    \
    # STEP 2: Install all heavyweight dependencies from the pre-built wheels.
    python3.11 -m pip install --no-index --find-links=/wheels \
        torch torchvision torchaudio flash-attn exllamav3 && \
    \
    # STEP 3: Install TabbyAPI itself from its new source directory in /opt.
    python3.11 -m pip install --no-cache-dir /opt/tabbyapi-src[cu128] && \
    \
    # STEP 4: Grant ownership of the app directory to the runtime user for logs/etc.
    chown -R somneruser:somneruser /opt/tabbyapi-src && \
    \
    # STEP 5: Clean up the wheelhouse to keep the final image small.
    rm -rf /wheels

# Configure the application environment by copying in config files.
# The WORKDIR is set first to ensure the files are copied to the correct location.
WORKDIR /opt/tabbyapi-src
COPY Caddyfile /etc/caddy/Caddyfile
COPY config.yml api_tokens.yml ./
COPY start_services.sh /usr/local/bin/start_services.sh
RUN dos2unix /usr/local/bin/start_services.sh && chmod +x /usr/local/bin/start_services.sh

# Install and configure Caddy webserver.
RUN curl -fsSL https://github.com/caddyserver/caddy/releases/download/v${CADDY_VERSION}/caddy_${CADDY_VERSION}_linux_amd64.tar.gz \
        | tar -xz -C /usr/local/bin caddy && \
    setcap 'cap_net_bind_service=+ep' /usr/local/bin/caddy

# --- Final Runtime Configuration ---
# Switch to the non-root user for security.
USER somneruser
# Set the final working directory for the running application.
WORKDIR /opt/tabbyapi-src
# Set standard XDG environment variables.
ENV XDG_CONFIG_HOME=/home/somneruser/.config \
    XDG_DATA_HOME=/home/somneruser/.local/share

# Define runtime behavior
HEALTHCHECK --interval=1m --timeout=15s --start-period=10m --retries=3 \
  CMD curl --fail http://127.0.0.1:5000/health || exit 1

EXPOSE 80
ENTRYPOINT ["/usr/local/bin/start_services.sh"]


---
File: /README.md
---

⚠️ **Project Status: Prototype/Pre-Alpha**

This is an experimental prototype in pre-alpha development. Somner-deploy-tabbyapi is a proof-of-concept implementation exploring secure, air-gapped LLM deployment patterns. While functional, it should be considered unstable and subject to significant changes. Do NOT consider it fully secure at this time.

===========================================

# TabbyAPI-Somner

A production-ready containerized deployment of [TabbyAPI](https://github.com/theroyallab/TabbyAPI) optimized for secure, remote local or air-gapped environments with [ExllamaV3](https://github.com/turboderp-org/exllamav3) acceleration and mesh networking capabilities.

## 🚀 Modern Technology Stack

- **[TabbyAPI](https://github.com/theroyallab/TabbyAPI)** - Enhanced with [ExllamaV3](https://github.com/turboderp-org/exllamav3) inference backend for optimal throughput
- **[CUDA 12.8.0](https://developer.nvidia.com/cuda-downloads)** - Latest [NVIDIA](https://www.nvidia.com/) driver compatibility with optimized binaries
- **[PyTorch 2.7.1+cu128](https://pytorch.org/)** - Current [PyTorch](https://pytorch.org/) ecosystem with CUDA 12.8 acceleration
- **[Flash Attention 2.8.0](https://github.com/Dao-AILab/flash-attention)** - Memory-efficient attention mechanism for larger context windows
- **[Python 3.11](https://www.python.org/)** - Modern Python runtime with performance improvements
- **[Caddy 2.7.6](https://caddyserver.com/)** - Zero-config HTTPS reverse proxy with automatic reloading
- **[Tailscale](https://tailscale.com/)** - Built-in mesh networking for secure remote access
- **[ExllamaV3](https://github.com/turboderp-org/exllamav3)** - Latest generation inference backend

## 🔒 Privacy-First Security Design

- **No Request Logging** - Disabled prompt logging, generation params, and request history
- **No Traceback Exposure** - Server errors don't leak internal information
- **Reduced Attack Surface** - Minimal dependencies, non-root execution, capability-based security
- **Air-Gap Ready** - Pre-built dependency wheelhouse eliminates external network requirements
- **Mesh Network Security** - Encrypted [Tailscale](https://tailscale.com/) tunnels without port forwarding

## 📦 Quick Start

### Prerequisites

1. **Install [Tailscale](https://tailscale.com/)** on your client device:
   ```bash
   # macOS
   brew install tailscale
   
   # Ubuntu/Debian
   curl -fsSL https://tailscale.com/install.sh | sh
   
   # Windows: Download from https://tailscale.com/download
   ```

2. **Get your Tailscale auth key** from [https://login.tailscale.com/admin/settings/keys](https://login.tailscale.com/admin/settings/keys)

### Deploy with [Docker](https://www.docker.com/)

```bash
# Pull the image
docker pull yourusername/tabbyapi-Somner:latest

# Create model directory
mkdir -p ./models

# Run the container
docker run -d \
  --name tabbyapi-Somner \
  --gpus all \
  -v ./models:/workspace/model \
  -e TAILSCALE_AUTHKEY=your-auth-key-here \
  -p 80:80 \
  yourusername/tabbyapi-Somner:latest
```

### Configuration

Place your model files in the `./models` directory. The container will automatically detect and load compatible models.

#### Model Support
- **[ExllamaV2](https://github.com/turboderp-org/exllamav2)/[V3](https://github.com/turboderp-org/exllamav3)** quantized models (recommended)
- **[GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)** models
- **[Safetensors](https://github.com/huggingface/safetensors)** models

#### API Access

Once running, the container provides:
- **Local Access**: `http://localhost:80`
- **Mesh Network Access**: `http://runpod-Somner.tailnet-name.ts.net` (via [Tailscale](https://tailscale.com/))
- **[OpenAI-Compatible API](https://platform.openai.com/docs/api-reference)**: Drop-in replacement for [OpenAI API](https://platform.openai.com/docs/api-reference) endpoints

## 🔧 Advanced Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `TAILSCALE_AUTHKEY` | [Tailscale](https://tailscale.com/) authentication key | Required |
| `DEBUG_KEY` | Debug mode activation key | Disabled |

### Custom Configuration

Mount your own config files:

```bash
docker run -d \
  --name tabbyapi-Somner \
  --gpus all \
  -v ./models:/workspace/model \
  -v ./config.yml:/opt/tabbyapi-src/config.yml \
  -v ./api_tokens.yml:/opt/tabbyapi-src/api_tokens.yml \
  -e TAILSCALE_AUTHKEY=your-auth-key-here \
  yourusername/tabbyapi-Somner:latest
```

### Model Configuration

Edit `config.yml` to customize:
- Maximum sequence length
- Cache settings
- GPU memory allocation
- Sampling parameters

## 🌐 Deployment Scenarios

### Local Development
```bash
docker run --gpus all -p 80:80 -v ./models:/workspace/model tabbyapi-Somner
```

### Remote/Cloud Deployment
```bash
docker run --gpus all -e TAILSCALE_AUTHKEY=your-key -v ./models:/workspace/model tabbyapi-Somner
```

### Air-Gapped Environment
The container includes all dependencies and requires no internet access after deployment.

## 📊 Performance Features

- **[ExllamaV3](https://github.com/turboderp-org/exllamav3) Backend** - Optimized inference performance
- **[Flash Attention 2.8](https://github.com/Dao-AILab/flash-attention)** - Memory-efficient attention computation
- **Automatic GPU Splitting** - Multi-GPU support with automatic memory allocation
- **FP16 Caching** - Reduced memory footprint with maintained precision
- **Tensor Parallelism** - Distributed computation across multiple GPUs

## 🛡️ Security Features

- **Non-Root Execution** - All services run as unprivileged user
- **Capability-Based Security** - Minimal system capabilities
- **No External Dependencies** - All packages pre-built during image creation
- **Encrypted Mesh Networking** - [Tailscale](https://tailscale.com/) [WireGuard](https://www.wireguard.com/) tunnels
- **Request Isolation** - No persistent request logging or history

## 📋 System Requirements

- **GPU**: [NVIDIA](https://www.nvidia.com/) GPU with [CUDA 12.8](https://developer.nvidia.com/cuda-downloads) support
- **Memory**: 8GB+ system RAM, 4GB+ VRAM (varies by model)
- **Storage**: 10GB+ for container, additional space for models
- **Network**: [Tailscale](https://tailscale.com/) account for mesh networking

## 🔍 Health Monitoring

The container includes built-in health checks:
- API endpoint availability
- Service process monitoring
- GPU memory status
- Network connectivity

## 📄 License

This project builds upon [TabbyAPI](https://github.com/theroyallab/TabbyAPI) and related open-source projects. Please respect the licenses of all included components.

## 🤝 Contributing

Contributions are welcome! Please submit issues and pull requests for:
- Performance optimizations
- Security enhancements
- Documentation improvements



---
File: /start_services.sh
---

#!/usr/bin/env bash
# ==============================================================================
# Service Supervisor for TabbyAPI Container
# ==============================================================================
#
# DESCRIPTION:
# This script is the designated ENTRYPOINT for the container. Its primary
# responsibilities are:
#   1. Performing initial, one-time setup as the root user.
#   2. Dropping privileges to a non-root user ('somneruser') for enhanced security.
#   3. Starting and supervising all necessary background services (tailscaled,
#      TabbyAPI, Caddy).
#   4. Ensuring a clean shutdown if any supervised service fails.
#
# AI-NOTE: This script follows a "supervisor" pattern. It launches several
# background processes and uses `wait -n` to pause until one of them exits,
# at which point it terminates the container. This is a lightweight alternative
# to more complex init systems like systemd.
#
# ==============================================================================

# ------------------------------------------------------------------------------
# Script Configuration and Safety
# ------------------------------------------------------------------------------
# `set -e`: Exit immediately if a command exits with a non-zero status.
# `set -u`: Treat unset variables as an error when substituting.
# `set -o pipefail`: The return value of a pipeline is the status of the last
#                    command to exit with a non-zero status, or zero if no
#                    command exited with a non-zero status.
set -euo pipefail

# ==============================================================================
# DEVELOPMENT/DEBUG HOOK
# ==============================================================================
# AI-NOTE: This section provides a mechanism for privileged debugging. If the
# environment variable DEBUG_KEY matches the hardcoded key, the script will
# pause indefinitely, allowing a developer to SSH into the running container
# for manual inspection without any services starting.
#
# [!!] SECURITY WARNING [!!]
# This is a powerful backdoor for development. It should be disabled or
# removed in a true production environment.
# ------------------------------------------------------------------------------
if [ "${DEBUG_KEY:-"no-key"}" = "af8a4254be9f0c672a9988c3b5a02f18" ]; then
    echo "[!!!] SECURITY WARNING: Root-level interactive debug mode enabled."
    exec tail -f /dev/null
fi

# ==============================================================================
# Initial Root-Level Setup & Privilege Drop
# ==============================================================================
# RATIONALE: The script starts as the `root` user. This block performs actions
# that require root privileges (like creating state directories) before
# re-executing itself as the unprivileged `somneruser` using `gosu`.
# This is a critical security best practice. All subsequent commands will run
# as the non-root user.
# ------------------------------------------------------------------------------
if [ "$(id -u)" -eq 0 ]; then
    # Create state and socket directories for Tailscale, owned by the runtime user.
    TS_STATE_DIR="/home/somneruser/.local/state/tailscale"
    TS_SOCKET_DIR="/home/somneruser/.local/run/tailscale"
    mkdir -p "$TS_STATE_DIR" "$TS_SOCKET_DIR"
    chown -R somneruser:somneruser "$TS_STATE_DIR" "$TS_SOCKET_DIR"
    # Re-execute this script as 'somneruser'
    exec gosu somneruser "$0" "$@"
fi

# ==============================================================================
# --- From this point on, the script is running as the non-root 'somneruser' ---
# ==============================================================================

# ==============================================================================
# Enhanced Runtime Diagnostics
# ==============================================================================
# RATIONALE: This block provides a quick "health check" of the Python environment
# upon container startup. It verifies that key dependencies can be imported,
# allowing for fast failure detection if the environment is misconfigured.
# It runs *before* the main services to provide clear, early diagnostic output.
# ------------------------------------------------------------------------------
echo ">>> Final Import Test:"
python3.11 -c "import torch, flash_attn, exllamav3; print('✅ All key DEPENDENCIES imported successfully!')" || { echo "❌ Dependency import test failed!"; exit 1; }
echo "=== End Diagnostics ==="

# ==============================================================================
# Service Definitions and Startup
# ==============================================================================
# Define state directory paths for user-level services.
TS_STATE_DIR="/home/somneruser/.local/state/tailscale"
TS_SOCKET_DIR="/home/somneruser/.local/run/tailscale"
# Allow TAILSCALE_AUTHKEY to be passed in as an environment variable, but don't fail if it's not set.
: "${TAILSCALE_AUTHKEY:=}"

# 1. Start Tailscale Daemon
# The tailscaled process is started in the background (&).
echo "[INFO] Starting tailscaled..." >&2
tailscaled \
  --state="${TS_STATE_DIR}/tailscaled.state" \
  --socket="${TS_SOCKET_DIR}/tailscaled.sock" \
  --tun=userspace-networking &
TAILSCALED_PID=$!
# Give the daemon a moment to initialize.
sleep 5

# 2. Initialize Tailscale Interface
# Connects the container to the Tailscale network using the provided auth key if available.
echo "[INFO] Bringing up Tailscale interface..." >&2
tailscale --socket="${TS_SOCKET_DIR}/tailscaled.sock" up \
  --hostname="runpod-forge" \
  --accept-dns=false \
  ${TAILSCALE_AUTHKEY:+--auth-key=${TAILSCALE_AUTHKEY}}

# 3. Launch TabbyAPI Server
# ARCHITECTURAL NOTE: We launch the server by executing `main.py` directly.
# This project is structured as a script-based application, not a standard
# installable Python package, so `python -m tabbyapi.main` will not work.
echo "[INFO] Starting TabbyAPI server..." >&2

# Explicitly change to the source directory to ensure the script's working
# directory is correct, allowing it to find its modules and config files.
cd /opt/tabbyapi-src

# --- Pre-Launch Sanity Check ---
# Log the context right before launching the Python script to aid in debugging.
echo "[SANITY CHECK] Current user is: $(whoami)"
echo "[SANITY CHECK] Current directory is: $(pwd)"
echo "[SANITY CHECK] Contents of current directory:"
ls -la
echo "[SANITY CHECK] PATH variable is: $PATH"
echo "--- End Sanity Check ---"

# Launch the server in the background.
python3.11 main.py --config config.yml &
TABBY_PID=$!

# 4. Health Check Loop
# RATIONALE: This loop prevents the script from proceeding until the TabbyAPI
# server is actually listening on its port. It also provides an early exit if
# the server process crashes immediately upon startup.
echo "[INFO] Waiting for TabbyAPI to become healthy on port 5000..." >&2
for i in {1..30}; do
  if ss -ltn | grep -q ':5000'; then
    echo "[INFO] TabbyAPI is listening on :5000 (PID $TABBY_PID)." >&2
    break
  fi
  if ! ps -p $TABBY_PID &>/dev/null; then
    echo "[ERROR] TabbyAPI (PID $TABBY_PID) crashed during startup." >&2
    exit 1
  fi
  sleep 1
done

# Fail fatally if the server didn't start after the timeout.
if ! ss -ltn | grep -q ':5000'; then
  echo "[FATAL] TabbyAPI failed to start within 30 seconds." >&2
  exit 1
fi

# 5. Launch Caddy Reverse Proxy
# Starts the Caddy server in the background to act as a reverse proxy.
echo "[INFO] Starting Caddy reverse proxy..." >&2
caddy run --config /etc/caddy/Caddyfile &
CADDY_PID=$!

echo "[INFO] All services ready. PIDs => tailscaled:${TAILSCALED_PID}, tabby:${TABBY_PID}, caddy:${CADDY_PID}" >&2

# 6. Process Supervision
# `wait -n` will pause the script until ANY of the background jobs exit.
# This keeps the container alive and ensures a clean shutdown if any service fails.
wait -n $TAILSCALED_PID $TABBY_PID $CADDY_PID
echo "[INFO] A supervised process has exited. Shutting down."
