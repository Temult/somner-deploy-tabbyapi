# ==============================================================================
# TabbyAPI Configuration File
# ==============================================================================
#
# DESCRIPTION:
# This file controls the behavior of the TabbyAPI server, including networking,
# logging, and model loading parameters.
#
# AI-NOTE: This file is a primary interface for users to customize the server.
# Key parameters include `host`, `port`, `model_dir`, and `model_name`.
#

# ------------------------------------------------------------------------------
# SECTION: Network
# PURPOSE: Configures the server's host, port, and API-level network settings.
# ------------------------------------------------------------------------------
network:
  host: 0.0.0.0
  port: 5000
  disable_auth: false
  disable_fetch_requests: false
  send_tracebacks: false
  api_servers: ["OAI"]

# ------------------------------------------------------------------------------
# SECTION: Logging
# PURPOSE: Controls the verbosity of server logging. Useful for privacy and
#          performance tuning.
# ------------------------------------------------------------------------------
logging:
  log_prompt: false
  log_generation_params: false
  log_requests: false

# ------------------------------------------------------------------------------
# SECTION: Model
# PURPOSE: Defines the primary model to be loaded and its runtime parameters.
#          This is the most critical section for performance and memory tuning.
# ------------------------------------------------------------------------------
model:
  # PARAMETER: model_dir
  # The base directory where model folders are stored. The server will look
  # for the `model_name` directory inside this path.
  # RATIONALE (RunPod): This is set to `/workspace/models` to utilize the
  # persistent volume, separating large model files from the container image.
  model_dir: /workspace/models

  # PARAMETER: model_name
  # The name of the specific model folder to load, located within `model_dir`.
  model_name: Doctor-Shotgun_L3.3-70B-Magnum-Diamond-EXL3

  # PARAMETER: max_seq_len
  # The maximum sequence length (context window) the model will support.
  # NOTE: Higher values consume significantly more VRAM, especially for the KV cache.
  # Ensure this value is compatible with the chosen model's architecture.
  max_seq_len: 32768

  # PARAMETER: rope_scale / rope_alpha
  # These parameters adjust RoPE scaling for models that require it to reach
  # a longer context window than they were trained for.
  # NOTE: Leave commented out to use the model's default values from its config.json.
  # rope_scale:
  # rope_alpha:

  # PARAMETER: use_dummy_models
  # If true, the server will start without loading a real model into VRAM.
  # Useful for testing the API server itself without requiring a GPU.
  use_dummy_models: false
  
  # PARAMETER: cache_mode
  # Determines the data type used for the KV (attention) cache.
  # OPTIONS: "FP16" (full precision), "Q4" (4-bit quantized).
  # RATIONALE: Using "Q4" dramatically reduces the VRAM required for the
  # context window (by ~75% compared to FP16), making large context
  # sizes on VRAM-constrained GPUs feasible. This is the key setting
  # that enables a 32k context on a 48GB card for a 70B model.
  cache_mode: Q4

  # PARAMETER: chunk_size
  # The size of chunks for processing prompts. Can be lowered to reduce VRAM
  # usage during prompt ingestion at a slight cost to performance.
  chunk_size: 2048
  
  # PARAMETER: cache_size
  # The size of the KV cache in tokens.
  # NOTE: By default (commented out), this will be set to `max_seq_len`.
  # cache_size:

  # --- Non-GPU specific settings ---
  inline_model_loading: false
  dummy_model_names: ["gpt-3.5-turbo"]
  use_as_default: []
  tensor_parallel: false
  gpu_split_auto: true
  autosplit_reserve: [96]
  gpu_split: []
  prompt_template:
  vision: false

# ------------------------------------------------------------------------------
# SECTION: Draft Model
# PURPOSE: Configuration for an optional, smaller "draft" model used for
#          speculative decoding to speed up generation. (Advanced feature).
# ------------------------------------------------------------------------------
draft_model:
  draft_model_dir: models
  draft_model_name:
  draft_rope_scale: 1.0
  draft_rope_alpha:
  draft_cache_mode: FP16
  draft_gpu_split: []

# ------------------------------------------------------------------------------
# SECTION: LoRA
# PURPOSE: Configuration for loading LoRA (Low-Rank Adaptation) adapters to
#          modify the behavior of the base model.
# ------------------------------------------------------------------------------
lora:
  lora_dir: loras
  loras:

# ------------------------------------------------------------------------------
# SECTION: Embeddings
# PURPOSE: Configuration for loading a dedicated model for generating text
#          embeddings.
# ------------------------------------------------------------------------------
embeddings:
  embedding_model_dir: models
  embeddings_device: cpu
  embedding_model_name:

# ------------------------------------------------------------------------------
# SECTION: Sampling
# PURPOSE: Allows for overriding default sampling parameters from a preset file.
# ------------------------------------------------------------------------------
sampling:
  override_preset:

# ------------------------------------------------------------------------------
# SECTION: Developer
# PURPOSE: Contains experimental or debugging flags. Use with caution.
# ------------------------------------------------------------------------------
developer:
  unsafe_launch: false
  disable_request_streaming: false
  cuda_malloc_backend: false
  realtime_process_priority: false